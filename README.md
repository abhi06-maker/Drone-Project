# Drone-Project
AI Drone Gesture Control with Human Follow-Me System

This project is an AI-based drone control simulation that allows control using hand gestures and includes an advanced human follow-me tracking feature.

The full logic is demonstrated on a computer screen (no physical drone required), but the system is designed for real drone integration in the future.

âœ¨ Main Features

Real-time hand gesture recognition using MediaPipe.

Separate custom CNN model trained on a user-collected gesture dataset.

Supports drone-style commands:

Takeoff

Land

Left / Right / Up / Down

Stop

Thumbs Up / Thumbs Down

Smart Follow-Me AI system:

Detects a specific human target.

Continuously tracks movement.

Maintains safe distance automatically.

Hovers when the person stops.

Increases speed when the person moves fast.

Works as a complete drone behavior simulation on screen.

ğŸ§  Project Novelty

Uses a fully custom gesture dataset, not default pretrained gestures.

Combines gesture control + intelligent human tracking in one system.

Target human can be selected using a gesture instead of face recognition.

mediapipe/
â”‚
â”œâ”€â”€ dataset/                  # Custom gesture images
â”œâ”€â”€ gesture_model.task        # Trained MediaPipe model
â”œâ”€â”€ detect.py                 # Gesture recognition (MediaPipe)
â”œâ”€â”€ final_drone.py            # Gesture + Follow-Me integration
â”‚
cnn/
â”‚
â”œâ”€â”€ split_data/               # Train / Validation / Test folders
â”œâ”€â”€ train_cnn_custom.py       # CNN training script
â”œâ”€â”€ realtime_cnn_detect.py    # Real-time CNN detection
â”œâ”€â”€ custom_gesture_model.keras

Technologies Used

Python

OpenCV

MediaPipe

TensorFlow / Keras

NumPy

Matplotlib

How to Run the Project
1ï¸âƒ£ MediaPipe Gesture Detection
python detect.py

2ï¸âƒ£ CNN Real-Time Gesture Detection

Activate environment and run:
python realtime_cnn_detect.py

3ï¸âƒ£ Final Drone Simulation (Gesture + Follow-Me)
python final_drone.py

Demo Flow

Start the camera feed.

Show a gesture â†’ command gets recognized.

Drone movement is simulated on screen.

Select a human target â†’ drone begins following.

If the person stops â†’ drone hovers automatically.

Future Improvements

Integration with a real drone.

Multi-person tracking with identity selection.

Deployment on Raspberry Pi / Jetson Nano.

Adding voice + gesture hybrid control.

ğŸ‘¨â€ğŸ’» Author

Developed as an AI + Computer Vision project demonstrating:

Gesture-based drone control with intelligent human tracking.

License

This project is created for educational and research purposes only.
Demonstrates real drone-level logic without physical hardware.

Designed for easy upgrade to real drone hardware in future.
